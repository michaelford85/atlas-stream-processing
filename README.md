# Atlas Stream Processing Demonstrations

This repository showcases practical demonstrations of **MongoDB Atlas Stream Processing (ASP)**, highlighting how to build real-time data pipelines that connect event streams (e.g., Kafka, AWS Kinesis) with MongoDB Atlas.

---

## ğŸŒŠ Overview

**MongoDB Atlas Stream Processing (ASP)** enables continuous processing of data in motion â€” allowing you to consume from event sources like **Kafka**, perform real-time transformations and aggregations, and emit the results to **Atlas**, **Kinesis**, or other sinks.

This repository includes examples designed to illustrate:
- **Kafka â†’ ASP â†’ MongoDB Atlas**: Ingest, enrich, and persist real-time events.
- **Kafka â†’ ASP â†’ Kinesis**: Stream processed data to downstream systems.
- **Real-time analytics** with tumbling windows, joins, and aggregations.
- **Error handling** using dead-letter queues.
- **Schema validation** and event enrichment in-flight.

---

## ğŸ§© Repository Structure

```
atlas-stream-processing/
â”œâ”€â”€ README.md
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ kafka_to_atlas.json
â”‚   â”œâ”€â”€ kafka_to_kinesis.json
â”‚   â”œâ”€â”€ tumbling_window_analytics.json
â”‚   â””â”€â”€ schema_validation.json
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ orders_sample.json
â”‚   â”œâ”€â”€ users_sample.json
â”‚   â””â”€â”€ pageviews_sample.json
â”œâ”€â”€ connections/
â”‚   â”œâ”€â”€ kafka_connection.json
â”‚   â”œâ”€â”€ atlas_connection.json
â”‚   â””â”€â”€ kinesis_connection.json
â”œâ”€â”€ demos/
â”‚   â”œâ”€â”€ kafka_to_atlas/
â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â””â”€â”€ demo_setup.sh
â”‚   â””â”€â”€ kafka_to_kinesis/
â”‚       â”œâ”€â”€ README.md
â”‚       â””â”€â”€ demo_setup.sh
â””â”€â”€ scripts/
    â”œâ”€â”€ generate_kafka_data.sh
    â”œâ”€â”€ generate_kinesis_data.sh
    â””â”€â”€ cleanup_resources.sh
```

---

## ğŸš€ Quick Start

### 1. Prerequisites
- A **MongoDB Atlas cluster** (M10 or higher)
- **Atlas Stream Processing** workspace enabled
- Access to a **Kafka** or **Confluent Cloud** cluster (for sample data)
- Optional: **AWS Kinesis** for sink demonstration

### 2. Create Connection Registries
Register your data sources and sinks in the ASP Connection Registry:
```bash
connections/
â”œâ”€â”€ kafka_connection.json
â”œâ”€â”€ atlas_connection.json
â””â”€â”€ kinesis_connection.json
```

### 3. Deploy a Sample Pipeline
Example: ingest `orders` data from Kafka and write to an Atlas collection.
```javascript
[
  {
    $source: {
      connectionName: "confluent-kafka",
      topic: "orders",
      timeField: { path: "eventTime" }
    }
  },
  { $addFields: { ingestedAt: $$NOW } },
  {
    $emit: {
      connectionName: "atlas-target",
      db: "demo",
      coll: "orders_streamed"
    }
  }
]
```

Upload this pipeline via the **Atlas UI** or **Atlas CLI**:
```bash
atlas streams pipelines create --file pipelines/kafka_to_atlas.json
```

---

## ğŸ§ª Sample Data Generators

Use one of the included scripts to simulate live data streams:

### Kafka
```bash
./scripts/generate_kafka_data.sh
```
Uses Confluent Cloudâ€™s **Datagen Source Connector** to publish mock `orders`, `users`, and `pageviews` events.

### Kinesis
```bash
./scripts/generate_kinesis_data.sh
```
Uses the **Amazon Kinesis Data Generator (KDG)** to send mock JSON events to your stream.

---

## ğŸ“Š Example Use Cases

| Scenario | Description |
|-----------|--------------|
| **Real-Time Order Tracking** | Enrich incoming order events with customer metadata before persisting to Atlas. |
| **Clickstream Aggregation** | Aggregate pageviews per user in a 30-second tumbling window. |
| **Cross-Stream Join** | Join Kafka topics `orders` and `users` to create a unified analytics view. |
| **Error Routing** | Send invalid messages to a Dead Letter Queue (DLQ) for inspection. |

---

## ğŸ§± Technologies Used
- **MongoDB Atlas Stream Processing**
- **Apache Kafka / Confluent Cloud**
- **AWS Kinesis**
- **MongoDB Atlas Database**
- **Bash & JSON pipeline definitions**

---

## ğŸ“š Learn More
- [MongoDB Atlas Stream Processing Documentation](https://www.mongodb.com/docs/atlas/stream-processing/)
- [Kafka $source Reference](https://www.mongodb.com/docs/atlas/stream-processing/stages/source/)
- [Kinesis $emit Reference](https://www.mongodb.com/docs/atlas/stream-processing/stages/emit/)
- [Confluent Cloud Datagen Connector](https://docs.confluent.io/cloud/current/connectors/cc-datagen.html)
- [AWS Kinesis Data Generator](https://awslabs.github.io/amazon-kinesis-data-generator/web/producer.html)

---

## ğŸ§  Contributing
Pull requests are welcome!  
If youâ€™d like to contribute new pipelines, data generators, or visualization demos, please open a PR with:
- A new folder under `/demos`
- Corresponding JSON pipeline(s)
- A `README.md` explaining the demoâ€™s flow and purpose

---

## ğŸ“ License
This project is licensed under the **Apache 2.0 License** â€” see the [LICENSE](LICENSE) file for details.
